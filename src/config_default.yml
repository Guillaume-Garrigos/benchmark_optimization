# config file
problem: # parameters about the problem to solve
    type: classification # type of problem, supports 'classification' and 'regression'.
    loss: Logistic # str, loss function name; support "Logistic" for classification; "L2" or "PseudoHuber" for regression
    regularizer: L2 # str, regularizer type name; support "L2" or "PseudoHuber"
    reg_parameter: inv_sqrt_data # the regularization parameter multiplying the regularizer.
    dataset_names: # a list of dataset names (stored as name.txt)
        - dummy
    dataset_path_folder: datasets # folder where the dataset are stored
    # a few more optional parameters, about the exact solution of the problem, which is computed with scipy.optimize.fmin_l_bfgs_b, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html#scipy.optimize.fmin_l_bfgs_b
    solution_path_folder: datasets # folder where the solution of the problem is stored
    save_solution: False # Should we compute and save the exact solution of the current problem? 
    load_solution: False # Should we load the exact solution of the current problem? If True will skip loading which saves time. Will look for a solution in the above indicated folder. Beware: if there is more than one solution corresponding to the parameters of the problem, we will pick the first one encountered
    pgtol: 1e-10 # pgtol parameter of scipy.optimize.fmin_l_bfgs_b
    factr: 1e7 # factr parameter of scipy.optimize.fmin_l_bfgs_b. 10.0 for extreme precision

    
results: # parameters about what to do with the experiments
    output_path: output # folder path to store experiments results
    xp_name: default experiment name # name of the current experiment. Used as a subfolder name
    measure_time: False  # do we measure time elapsed for each epoch? If so, each quantity to be plotted will be plot with respect to both [number of epochs] and [time].
    save_data: True # do we save the results?
    save_plot: True # do we save the plot as pdf files?
    do_we_plot: True # do we plot anything at all?
    grid_search_comparison_plot: True # in case of grid search do we want to plot a specific graph comparing them?
    grid_search_curves_plot: False # in case of grid search do we want to plot the plenty curves for each record?
    log_file: log.log # log file name 
    
    dpi: 50 # quality of the produced graphs. Strong impact on performance.
    figsize: [18, 12] # size of the figure. The larger the cleaner?
    plot_threshold_down: null # value under which we stop plotting results (typically machine precision, but also for handling cases when one solver converges way too fast with respect to the others). None if we don't want to threshold at all.
    plot_threshold_up: null # same but upper threshold. None by default
    absolute_values: False # if True, turn all values recorded into absolute values. Useful if we suspect that some values are epsilon-negative and this ruins a log plot.
    use_solution: True # Should we use the "exact" solution of the problem for plotting? Typically for using inf f when plotting f(x) - inf f. Can also be use to set parameters of some algorithms. Setting True implies that the solution will be computed (or loaded)
    show_variance: True # when running repetitions, show variance for the results
    variance_type: 'minmax' # can be std
    latex: True # SHould we activate latex for the fonts? Allows to write more math. True by default, but makes the first plot a bit slow ...
    records_to_plot: # list. Typically gradient_norm or function_splitting. see Records for more details
        - gradient_norm
    
solvers_parameters: # general parameters applied to all solvers. can be overwritten by specific solver parameters below
    nb_repetition: 2 # number of repetitions run for stochastic algorithm
    nb_epochs: 10 # number of epochs to run for one algorithm, i.e., effective data passes
    tolerance: 1e-8 # float, the algorithms will be stopped when the norm of gradient reaches below this threshold.
    initialization: zeros # how to initialize the algorithm. Only zeros implemented so far
    distribution: uniform # for some algorithms, we need to sample data wrt a distribution. Can be "uniform". Other wise must be a lambda function taking nb_data as an input and returning a certain np.array
# example : lambda nb_data, p0=1./(nb_data + 1): np.array([p0] + [(1 - p0) / nb_data] * nb_data)
    stepsize_type: constant # could be vanishing
    stepsize_factor: 1.0 # float, multiplies the default stepsize of the method by this factor
    stepsize_vanishing_exponent: 0.5 # float, we divide the stepsize by t^exponent
    extrapolation_parameter: 1.0 # in [0,1], adds an extrapolation step to algorithms
    
solvers: # list of solvers and optionnally their specific parameters as a dict. can have duplicates here but name_result must differ
    - SGD
    