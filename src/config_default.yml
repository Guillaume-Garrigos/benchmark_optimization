# config file
problem: # parameters about the problem to solve
    type: classification # type of problem, supports 'classification' and 'regression'.
    loss: Logistic # str, loss function name; support "Logistic" for classification; "L2" or "PseudoHuber" for regression
    regularizer: L2 # str, regularizer type name; support "L2" or "PseudoHuber"
    reg_parameter: inv_sqrt_data # the regularization parameter multiplying the regularizer.
    dataset_names: # a list of dataset names (stored as name.txt)
        - dummy
    dataset_path_folder: datasets # folder where the dataset are stored
    
results: # parameters about what to do with the experiments
    output_path: output # folder path to store experiments results
    xp_name: config via yaml # name of the current experiment. Used as a subfolder name
    measure_time: False  # do we measure time elapsed for each epoch? If so, each quantity to be plotted will be plot with respect to both [number of epochs] and [time].
    save_data: True # do we save the results?
    verbose: True # Should we save the outputs (data and plots)? 1 for yes, 0 for no. Default is 1. CONFLICTS HERE, needs cleaning
    log_file: log.txt # log file name
    
    dpi: 50 # quality of the produced graphs. Strong impact on performance.
    plot_threshold: 1.0e-12 # value under which we stop plotting results (typically machine precision, but also for handling cases when one solver converges way too fast with respect to the others). None if we don't want to threshold at all.
    subopt: True # Should we plot sub-optimality curves?
    records_to_plot: # list. Typically gradient_norm or function_splitting. see Records for more details
        - gradient_norm
    
solvers_parameters: # general parameters applied to all solvers. can be overwritten by specific solver parameters below
    nb_repetition: 2 # number of repetitions run for stochastic algorithm
    nb_epochs: 100 # number of epochs to run for one algorithm, i.e., effective data passes
    stepsize_factor: 0.5 # float, multiplies the default stepsize of the method by this factor
    tolerance: 1e-8 # float, the algorithms will be stopped when the norm of gradient reaches below this threshold.
    initialization: zeros # how to initialize the algorithm. Only zeros implemented so far
    distribution: uniform # for some algorithms, we need to sample data wrt a distribution. Can be "uniform". Other wise must be a lambda function taking nb_data as an input and returning a certain np.array
# example : lambda nb_data, p0=1./(nb_data + 1): np.array([p0] + [(1 - p0) / nb_data] * nb_data)
    lr: 1.0 # float, learning rate value for SAN/SANA
    
solvers: # list of solvers and optionnally their specific parameters as a dict. can have duplicates here but name_result must differ
    - SAN:
        name_result: SAN_constant
        stepsize_factor: 1.5
        stepsize_type: constant
    - SAN:
        name_result: SAN_vanishing
        stepsize_factor: 1.5
        stepsize_type: vanishing
        stepsize_vanishing_exponent: 0.5
    - SGD
    