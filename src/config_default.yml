# config file
problem: # parameters about the problem to solve
    type: classification # type of problem, supports 'classification' and 'regression'.
    loss: Logistic # str, loss function name; support "Logistic" for classification; "L2" or "PseudoHuber" for regression
    regularizer: L2 # str, regularizer type name; support "L2" or "PseudoHuber"
    reg_parameter: inv_sqrt_data # the regularization parameter multiplying the regularizer.
    dataset_names: # a list of dataset names (stored as name.txt)
        - dummy
    dataset_path_folder: datasets # folder where the dataset are stored
    
results: # parameters about what to do with the experiments
    output_path: output # folder path to store experiments results
    xp_name: default experiment name # name of the current experiment. Used as a subfolder name
    measure_time: False  # do we measure time elapsed for each epoch? If so, each quantity to be plotted will be plot with respect to both [number of epochs] and [time].
    save_data: True # do we save the results?
    save_plot: True # do we save the plot as pdf files?
    do_we_plot: True # do we plot anything at all?
    grid_search_comparison_plot: True # in case of grid search do we want to plot a specific graph comparing them?
    grid_search_curves_plot: False # in case of grid search do we want to plot the plenty curves for each record?
    log_file: log.txt # log file name 
    
    dpi: 50 # quality of the produced graphs. Strong impact on performance.
    figsize: [18, 12] # size of the figure. The larger the cleaner?
    plot_threshold_down: 1.0e-12 # value under which we stop plotting results (typically machine precision, but also for handling cases when one solver converges way too fast with respect to the others). None if we don't want to threshold at all.
    plot_threshold_up: # same but upper threshold. None by default
    subopt: True # Should we plot sub-optimality curves?
    show_variance: True # when running repetitions, show variance for the results
    variance_type: 'minmax' # can be std
    latex: True # SHould we activate latex for the fonts? Allows to write more math. True by default, but makes the first plot a bit slow ...
    records_to_plot: # list. Typically gradient_norm or function_splitting. see Records for more details
        - gradient_norm
    
solvers_parameters: # general parameters applied to all solvers. can be overwritten by specific solver parameters below
    nb_repetition: 2 # number of repetitions run for stochastic algorithm
    nb_epochs: 10 # number of epochs to run for one algorithm, i.e., effective data passes
    tolerance: 1e-8 # float, the algorithms will be stopped when the norm of gradient reaches below this threshold.
    initialization: zeros # how to initialize the algorithm. Only zeros implemented so far
    distribution: uniform # for some algorithms, we need to sample data wrt a distribution. Can be "uniform". Other wise must be a lambda function taking nb_data as an input and returning a certain np.array
# example : lambda nb_data, p0=1./(nb_data + 1): np.array([p0] + [(1 - p0) / nb_data] * nb_data)
    stepsize_type: constant # could be vanishing
    stepsize_factor: 1.0 # float, multiplies the default stepsize of the method by this factor
    stepsize_vanishing_exponent: 0.5 # float, we divide the stepsize by t^exponent
    extrapolation_parameter: 1.0 # in [0,1], adds an extrapolation step to algorithms
    
solvers: # list of solvers and optionnally their specific parameters as a dict. can have duplicates here but name_result must differ
    - SGD
    